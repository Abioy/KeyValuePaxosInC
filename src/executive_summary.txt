Project 3 - TCSS 558 (Distributed Computing), UW Tacoma
Kevin Anderson & Daniel Kristiyanto

== Assignment Overview ==
In Project 2, we were given task to leverage the functionalities of RPC to simplify the communication methods between one machine to another. In this project, we move forward by expanding Project 2, from a simple client-server communication, into a distributed system. In this project, we replicate a key-value to 5 servers and implement 2 phase commit protocol to ensure data consistency. 

In this scenario, client can write a value to any server in the list, but before a server responds to a client’s requests to modify the data, it will first check with all other servers in the case that another request is currently being performed. If the item requested is currently locked due to other request, it will abort the request until the lock status is changed. In the scenario where the client only need to read, server does not do the checking, but instead, return with any value it currently holds.  

== Technical Impression ==
This project is an extension to the previous project. There is not much changes to the client functionalities that was assigned for the previous project, except some minor UI, removal of the TCP/UDP option (Project 1), and the addition of selecting desired server.

There are many significant modifications to the server functionalities. To ensure consistency, the server cannot simply answer to client’s request. Instead, we have implemented 2 Phase-Commit (2PC).  The server directly contacted by client automatically becomes a coordinator and responds to all neighboring nodes.  In order to do so, all nodes must know about all other nodes.  Managing this state could be difficult in larger systems, but for this toy example, it works quite well.  Consideration for election algorithms such as token-ring or bully were considered but were discarded due to time constraints.  For mutual exclusion, we are simply utilizing a lock on keys that are in the prepare state.  Again this works well for this toy example.  Implementation of shared-priority, or a mutex server were considered but also discarded due to complexity and time constraints.

In this 2PC communication, instead of using broadcast message, the coordinator iterates through a list of nodes, sending a PREPARE(X) command.  It then counts the number of READY(X) responses to the messages.  If a NACK(X) is received, the call will fail and the coordinator will respond to the client appropriately.   When a node receives a PREPARE(X) command,  the node read will read the current lock on key X.  If key X is currently locked (meaning there is a concurrent operation by itself or by other coordinator), a NACK(X) is sent, otherwise a lock will be placed on X and a READY(X) will be sent. If the coordinator receives any NACK(X) responses, an ABORT(X) command will be sent and any nodes in READY(X) state will remove the locks on X.

== Issues and Observations ==
We investigate that there are some scenarios when this algorithm may fail. For an instance in this scenario:
  1. 5 server nodes are listed (node 5-10), but only 2 are run (node 6 & 7). 
  2. Client request to deposit a value to node 6. 
  3. Node 6 becomes a coordinator in this case, iteratively contacting other 4 servers (node 7-10), but only received a response from node 7.
  4. Since it only receives a response from 2 out of 4, it fails the request. 

In this case, we learned that all nodes have to be completely responsive in order for the system running. A single failure in any of the nodes brings the entire system (or otherwise will compromise data consistency). This can be solved by removing the non-responding from the list periodically, and then making sure than newly-added node will first be synced before serving any request. Other solution is by implementing Paxos which will be on the next project.  This behavior is a known issue with 2PC.  It is NOT fault tolerant.

An issue with memory management using RPC.H has arisen, which we have not yet completely solved.  The method for responding to an RPC is hard set.  The input is provided a pointer to a struct, and the response message is a pointer to a struct, returned by the method.  This gives us two options.  Allocate memory on the call stack by creating a local variable and passing that address, or allocate main memory using malloc() and return that memory address.  Both contain issues.  Passing memory address of the call stack is of course problematic, because that memory can and will be overwritten on the next function call.  Allocating new memory is problematic because we cannot free that memory after it is sent.  Using xdr_free() we were able to ‘solve’ the situation by clearing an address each time the xdr_rpc function is called, but this opens yet another bug which can be explained in another paper.

Another bug that we have identified and will fix in the next iteration of the software is exposed when, in certain situations, locks are mishandled.   When a client requests data to be changed, the coordinator simply sets the lock, and doesn’t check the status first.  So, if the lock was set, and set ONLY on the coordinator two processes could potentially “own the lock” so to speak.  Example:  A put and delete are made at the same time on the same server.  If one thread is handing the delete and the other is handling the put, they will both send out messages to the neighboring nodes PREPARE(X).  It is very likely that both will receive at least one NACK(X) and both will abort.  If the locks were first checked, at least one of them (the first to the lock) would have been successful.

Lastly, there is an opportunity for improvement in the collection of messages, that will be implemented in project 4.  Currently, all N-1 servers are contacted, in serial.  A solution will be to spawn N-1 threads and contact the servers.  In 2PC we would hold for them all to return, but in paxos, we must only hold until we have a quorum.
