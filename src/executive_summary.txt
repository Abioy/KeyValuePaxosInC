Project 4 - TCSS 558 (Distributed Computing), UW Tacoma
Kevin Anderson & Daniel Kristiyanto

== Assignment Overview ==
2 phase commit that we implemented in the preceding project gave an assurance that we preserve consistency across our servers as long the entire array of the servers are up and running. In practice, this requirements may not be realistic. Paxos comes as a solution. 

Paxos, a popular algorithm in distributed system, is inspired from a parliamentary  system in Paxos, Greece. It is obvious where the name is from (unlike Byzantine Algorithm). The intuition of Paxos is to have a quorum to maintain consistency. Quorum is achieved based on majority (at least half of the number of servers plus one) votes. In this project, we upgraded from what we have in 2 Phase Commit with a major updates. RPC is still used for the communication method due to its simplicity in usage. However major changes are applied. 

== Technical Impression ==
Unlike in 2 PC, we now add some functionalities to our server that reflects to the requirements of Paxos: acceptor, listener, and proposers. We also add lamport clock to our message. 

Upon receiving request from client, proposers contacted all the acceptors across the server list and collect their responses. Should the proposer accept an okay response (or called as promises) from the majority of the servers, it'll then proceed to the next stage. In the second stage the similar mechanism is repeated, and should the proposers accept okay message (called accept) from the majority of the servers, then the commands will be passed for an execution by passing the commands to all learners. Learners comply with execution commands from the proposer. When quorum is not met at any of the stage, the proposer will abort, and send an error to client. 

For an acceptor to send a promise or accept message, it has the following conditions:
- The lamport clock from the proposer must not be less than acceptor's lamport clock. 
- A proposer must have completed all the stages successfully and in order. 

As for the get command, Paxos is not fully implemented. In get, proposers send a read message to all learners directly, collect the responses. Final response is decided by the quorum of the responses (majority voting). If the final response differ with the data stored in requested node, the node will learn and update its data to ensure consistency. 
Our codes are written in C, and inherits some of the structures from project 3, such as the use of serverlist.txt to store the information of the list of the servers. There is not much changes in the client sides. 

== Observations and challenges ==
Although it works well to maintain consistency and has more flexibilities than 2 PC, We observe that there are many security holes in this project. Any learner can confirm promises that are given to other learners (assuming that they know the detail such as the key and the stage of the server), since acceptors do not check for origin's messages. There are many other scenarios where the security can be exploited. 

Implementing Paxos was far easier experience than 2PC.  In 2PC, the algorithm was easier, but implementing the code was riddled with challenges.  For Paxos, once the algorithm was fully understood, the implementation went quite smoothly with very little debugging required.

We made a few design decisions that made implementation easier, and would have been different if this system wasn’t purely academic.  The first implementation decision is a loop construct for obtaining a quorum and not broadcast.  The issue with a loop is that if the first servers are slow to respond, it takes longer to obtain a quorum.  Similarly, in our design, the default timeouts for RPC are the same for the server to server communication as well as client to server.  This is an issue when it comes to delayed server responses.  Imagine Client C requests information from server S.  Server S then awaits a response from Server X.  Server X fails to respond and Server S’s message times out.  Because Clienc C’s timeout is the same, it will have already timed out waiting for a response from Server S.  This is the sole reason we killed the processes in the chaos_function rather than stalling a response.   The solution would have been to use broadcast, or at very least a similated multicast using pthreads.

Another implementation decision was putting the burden of retries on the client.  If a PUT is requested to a server with an old lamport clock, it will be denied.  The client must then retry.  When it does, the system should accept the new value, because its lamport clock should have been brought up to date with when it was previously denied.

In a sense, project 4 was the easiest of the projects in this class.  We consider that as evidence of significant learning and improved understanding of network programming and distributed systems in general.  This was a very fulfilling project and we hope to build upon it in the future.


